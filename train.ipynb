{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sumin\\Desktop\\development\\nomad_GPT\\FULLSTACK-GPT\\envGPT\\Lib\\site-packages\\langchain\\llms\\openai.py:216: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sumin\\Desktop\\development\\nomad_GPT\\FULLSTACK-GPT\\envGPT\\Lib\\site-packages\\langchain\\llms\\openai.py:811: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hi Sumin! How can I assist you today?'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4.6 serialization(직렬화, 데이터나 객체를 저장 가능한 형태 또는 전송 가능한 형태로 바꾸는 과정)\n",
    "# chat 설정 저장하는 방법, 불러오는 방법\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms.openai import OpenAI\n",
    "from langchain.llms.loading import load_llm\n",
    "\n",
    "# chat = OpenAI(\n",
    "#     temperature=0.1,\n",
    "#     max_tokens=450,\n",
    "#     model=\"gpt-4o-mini\"\n",
    "# )\n",
    "\n",
    "# 설정 저장하기\n",
    "# chat.save(\"model_gpt4_4o_mini.json\")\n",
    "\n",
    "# 설정 불러오기\n",
    "chat = load_llm(\"model_gpt4_4o_mini.json\")\n",
    "\n",
    "chat.predict(\"Hi! I'm Sumin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4.6 \n",
    "# OpenAI를 사용할 때 비용 측정 방법\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.globals import set_debug\n",
    "\n",
    "set_debug(False)\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "with get_openai_callback() as usage:\n",
    "    a = chat.predict(\"What is the recipe for sausage\")\n",
    "    b = chat.predict(\"What is the recipe for macaroon\")\n",
    "    print(a, b, \"\\n\")\n",
    "    print(usage)\n",
    "    print(\"total cost:\", usage.total_cost)\n",
    "    print(\"total token:\", usage.total_tokens)\n",
    "    print(\"prompt tokens:\", usage.prompt_tokens)\n",
    "    print(\"completion tokens:\", usage.completion_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4.5 유용한 기능 소개\n",
    "# set_llm_cache(InMemoryCache()): 답변을 저장함. 같은 질문을 받을 경우, 저장된 답변을 바로 출력함. 시스템을 껐다 켜면 사라짐.\n",
    "# set_llm_cache(SQLiteCache(\"원하는이름.db\")): 답변 저장. 파일로 저장하기 때문에 껐다 켜도 사용 가능.\n",
    "# 이 외에도 다양한 cache를 사용할 수 있음. langchain에서는 다양한 third party를 제공하고 있음. 또한, 직접 cache를 만들어서 쓸 수도 있음.\n",
    "# set_debug(True): llm에서 어떤 일이 일어나고 있는지 출력해줌. 답변이 나온 이유, 어떤 모델을 쓰고 있는지 등등.\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.globals import set_llm_cache, set_debug\n",
    "from langchain.cache import InMemoryCache, SQLiteCache\n",
    "\n",
    "# set_llm_cache(InMemoryCache())\n",
    "set_llm_cache(SQLiteCache(\"cache.db\"))\n",
    "set_debug(True)\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    # streaming=True,\n",
    "    # callbacks=[StreamingStdOutCallbackHandler(), ],\n",
    ")\n",
    "\n",
    "chat.predict(\"How do you make korea's Braised ribs\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#4.4 메모리 합치기(많은 프롬프트를 하나로 합치기)\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler(),],\n",
    ")\n",
    "\n",
    "intro = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a role playing assistant.\n",
    "    And you are impersonating a {character}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "example = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    This is an example of how you talk:\n",
    "\n",
    "    Human: {example_question}\n",
    "    You: {example_answer}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "start = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Start now!\n",
    "\n",
    "    Human: {question}\n",
    "    You:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "final = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    {intro}\n",
    "\n",
    "    {example}\n",
    "\n",
    "    {start}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "prompts = [\n",
    "    (\"intro\", intro),\n",
    "    (\"example\", example),\n",
    "    (\"start\", start),\n",
    "]\n",
    "\n",
    "full_prompt = PipelinePromptTemplate(final_prompt=final, pipeline_prompts=prompts)\n",
    "\n",
    "# full_prompt.format(\n",
    "#     character=\"Pirate\",\n",
    "#     example_question=\"What is your location?\",\n",
    "#     example_answer=\"Arrrg! That is a secret!\",\n",
    "#     question=\"What is your fav food?\",\n",
    "# )\n",
    "\n",
    "chain = full_prompt | chat\n",
    "\n",
    "# chain.invoke(\n",
    "{\n",
    "    \"character\":\"Pirate\",\n",
    "    \"example_question\":\"What is your location?\",\n",
    "    \"example_answer\":\"Arrrg! That is a secret!\",\n",
    "    \"question\":\"What is your fav food?\",\n",
    "}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#4.4 파일에서 형식 불러오기\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import load_prompt\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler(),],\n",
    ")\n",
    "\n",
    "# prompt = load_prompt(\"./prompt.json\")\n",
    "prompt = load_prompt(\"./prompt.yaml\")\n",
    "prompt.format(country=\"North Korea\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.example_selector.base import BaseExampleSelector\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler(), ]\n",
    ")\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What do you know about France?\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I Know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"what do you know about Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I Know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "class RandomExampleSelector(BaseExampleSelector):\n",
    "\n",
    "    def __init__(self, examples):\n",
    "        self.examples = examples\n",
    "    \n",
    "    def add_example(self, example):\n",
    "        self.examples.append(example)\n",
    "\n",
    "    def select_examples(self, input_variables):\n",
    "        from random import choice\n",
    "        return [choice(self.examples)]\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\"Human: {question}\\nAI: {answer}\")\n",
    "\n",
    "example_selector = RandomExampleSelector(\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    example_selector=example_selector,\n",
    "    suffix= \"What do you know about {country}?\", #형식화된 예제의 마지막에 나오는 내용\n",
    "    input_variables=[\"country\"]\n",
    ")\n",
    "\n",
    "prompt.format(country=\"Brazil\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler(), ]\n",
    ")\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What do you know about France?\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I Know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"what do you know about Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I Know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\"Human: {question}\\nAI: {answer}\")\n",
    "\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    max_length=180,\n",
    ")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    example_selector=example_selector,\n",
    "    suffix= \"What do you know about {country}?\", #형식화된 예제의 마지막에 나오는 내용\n",
    "    input_variables=[\"country\"]\n",
    ")\n",
    "\n",
    "prompt.format(country=\"Brazil\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['country'] messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a geography expert, you give short answer.')), FewShotChatMessagePromptTemplate(examples=[{'country': 'France', 'answer': '\\n        Here is what I know:\\n        Capital: Paris\\n        Language: French\\n        Food: Wine and Cheese\\n        Currency: Euro\\n        '}, {'country': 'Italy', 'answer': '\\n        I Know this:\\n        Capital: Rome\\n        Language: Italian\\n        Food: Pizza and Pasta\\n        Currency: Euro\\n        '}, {'country': 'Greece', 'answer': '\\n        I Know this:\\n        Capital: Athens\\n        Language: Greek\\n        Food: Souvlaki and Feta Cheese\\n        Currency: Euro\\n        '}], example_prompt=ChatPromptTemplate(input_variables=['answer', 'country'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['country'], template='What do you know about {country}?')), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['answer'], template='{answer}'))])), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['country'], template='What do you know about {country}?'))]\n",
      "\n",
      "        I Know this:\n",
      "        Capital: Bangkok\n",
      "        Language: Thai\n",
      "        Food: Pad Thai and Tom Yum\n",
      "        Currency: Thai Baht\n",
      "        "
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='\\n        I Know this:\\n        Capital: Bangkok\\n        Language: Thai\\n        Food: Pad Thai and Tom Yum\\n        Currency: Thai Baht\\n        ')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.few_shot import FewShotChatMessagePromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler(), ]\n",
    ")\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"country\": \"France\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Italy\",\n",
    "        \"answer\": \"\"\"\n",
    "        I Know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Greece\",\n",
    "        \"answer\": \"\"\"\n",
    "        I Know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"What do you know about {country}?\"),\n",
    "    (\"ai\", \"{answer}\")\n",
    "])\n",
    "\n",
    "example_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a geography expert, you give short answer.\"),\n",
    "    example_prompt,\n",
    "    (\"human\", \"What do you know about {country}?\")\n",
    "])\n",
    "\n",
    "print(final_prompt)\n",
    "\n",
    "chain = final_prompt | chat\n",
    "\n",
    "chain.invoke({\n",
    "    \"country\":\"Thailand\",\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler(), ]\n",
    ")\n",
    "# chat.predict(\"What do you know about India?\")\n",
    "\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What do you know about France?\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I Know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"what do you know about Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I Know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# 1. 예제 형식 지정하기(좀 더 명확하게)\n",
    "# example_template = \"\"\"\n",
    "#     Human: {question}\n",
    "#     AI: {answer}\n",
    "# \"\"\"\n",
    "\n",
    "# example_prompt = PromptTemplate.from_template(example_template)\n",
    "\n",
    "# 2. 예제 형식 지정하기(간단)\n",
    "example_prompt = PromptTemplate.from_template(\"Human: {question}\\nAI: {answer}\")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples, #example_prompt 와 examplaes를 가져와 형식화(formatting)함. \n",
    "    suffix= \"What do you know about {country}?\", #형식화된 예제의 마지막에 나오는 내용\n",
    "    input_variables=[\"country\"]\n",
    ")\n",
    "\n",
    "chain = prompt | chat\n",
    "\n",
    "chain.invoke({\n",
    "    \"country\":\"Turkey\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# PromptTemplate Using #1 ======================\n",
    "# t = PromptTemplate.from_template(\"What is the capital of {country}\")\n",
    "# t.format(country=\"India\")\n",
    "\n",
    "# PromptTemplate Using #2 ======================\n",
    "# t = PromptTemplate(\n",
    "#     template=\"What is the capital of {country}\",\n",
    "#     input_variables=[\"country\"],\n",
    "# )\n",
    "# t.format(country=\"India\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatPromptTemplate, chain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler(), ]\n",
    ")\n",
    "\n",
    "poet_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a poet. You don’t write anything other than poetry.\"),\n",
    "    (\"human\", \"Write me a poem. The theme of the poem is {language}.\")\n",
    "])\n",
    "\n",
    "poet_chain = poet_template | chat\n",
    "\n",
    "explain_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a critic. Read the poem and evaluate what the poem is like.\"),\n",
    "    (\"human\", \"{poem}\")\n",
    "])\n",
    "\n",
    "explain_chain = explain_template | chat\n",
    "\n",
    "final_chain = {\"poem\": poet_chain} | explain_chain\n",
    "\n",
    "final_chain.invoke({\n",
    "    \"language\":\"javascript\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='인도와 베트남 사이의 거리는 약 3,500km입니다. 제 이름은 수민이에요.')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are geography expert. And you only reply in {language}.\"),\n",
    "    (\"ai\", \"안녕, 나는 {name}!\"),\n",
    "    (\"human\", \"What is the distance between {country_a} and {country_b}. Also, What is your name?\")\n",
    "])\n",
    "\n",
    "prompt = template.format_messages(\n",
    "    language=\"Korean\",\n",
    "    name=\"Sumin\",\n",
    "    country_a=\"India\",\n",
    "    country_b=\"Vietnam\"\n",
    ")\n",
    "\n",
    "chat.predict_messages   (prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The distance between Seoul, South Korea and France varies depending on the specific cities being compared. For example, the distance between Seoul and Paris, France is approximately 9,500 kilometers (5,900 miles) when measured in a straight line. However, the actual distance traveled by air or land would be longer due to the curvature of the Earth and the specific flight path taken.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts importPromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "template = PromptTemplate.from_template(\n",
    "    \"What is the distance between {country_a} and {country_b}?\"\n",
    ")\n",
    "\n",
    "prompt = template.format(country_a=\"seoul\", country_b=\"France\")\n",
    "\n",
    "chat.predict(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Here is a simple recipe for French Onion Soup:\\n\\nIngredients:\\n- 4 large onions, thinly sliced\\n- 4 tablespoons butter\\n- 2 tablespoons olive oil\\n- 2 cloves garlic, minced\\n- 1 teaspoon sugar\\n- 1/2 cup dry white wine\\n- 6 cups beef broth\\n- Salt and pepper to taste\\n- Baguette slices\\n- Gruyere cheese, grated\\n\\nInstructions:\\n1. In a large pot, melt the butter and olive oil over medium heat. Add the sliced onions and cook, stirring occasionally, until caramelized and golden brown, about 30-40 minutes.\\n2. Add the minced garlic and sugar to the onions and cook for another 2-3 minutes.\\n3. Pour in the white wine and cook for a few minutes to deglaze the pot.\\n4. Add the beef broth to the pot and bring to a simmer. Season with salt and pepper to taste.\\n5. Preheat the broiler. Ladle the soup into oven-safe bowls and top each with a slice of baguette and a generous amount of grated Gruyere cheese.\\n6. Place the bowls under the broiler until the cheese is melted and bubbly, about 2-3 minutes.\\n7. Serve hot and enjoy your delicious French Onion Soup!\\n\\nFeel free to adjust the seasonings and cheese to your liking. Enjoy!')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are specialist cooker\"),\n",
    "    AIMessage(content=\"Hi, I'm Sumin\"),\n",
    "    HumanMessage(content=\"What is the receipe of onion soup?\")\n",
    "]\n",
    "\n",
    "chat.predict_messages(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Hi!\\nAI: How are you\\nHuman: Hi!\\nAI: How are you2\\nHuman: Hi!\\nAI: How are you3'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5.0 ConversationBufferMemory\n",
    "# 대화를 모두 저장\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=False)\n",
    "#return_message: True(Chat모델 return 해줌.) / False(string 형태로 return 해줌.)\n",
    "\n",
    "memory.save_context({\"input\":\"Hi!\"}, {\"output\":\"How are you\"})\n",
    "\n",
    "memory.load_memory_variables({})\n",
    "\n",
    "memory.save_context({\"input\":\"Hi!\"}, {\"output\":\"How are you2\"})\n",
    "\n",
    "memory.load_memory_variables({})\n",
    "\n",
    "memory.save_context({\"input\":\"Hi!\"}, {\"output\":\"How are you3\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='2'),\n",
       "  AIMessage(content='2'),\n",
       "  HumanMessage(content='3'),\n",
       "  AIMessage(content='3'),\n",
       "  HumanMessage(content='4'),\n",
       "  AIMessage(content='4'),\n",
       "  HumanMessage(content='5'),\n",
       "  AIMessage(content='5')]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5.1 ConversationBufferWindowMemory\n",
    "# 특정 개수까지 대화를 저장\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    return_messages=True, # 챗모델 형태로 메시지를 전달해줌.\n",
    "    k=4 # 4개까지 저장함.\n",
    ")\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "add_message(1,1)\n",
    "add_message(2,2)\n",
    "add_message(3,3)\n",
    "add_message(4,4)\n",
    "add_message(5,5)\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['history', 'question'] input_types={'history': typing.List[typing.Union[langchain.schema.messages.AIMessage, langchain.schema.messages.HumanMessage, langchain.schema.messages.ChatMessage, langchain.schema.messages.SystemMessage, langchain.schema.messages.FunctionMessage]]} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful AI talking to a human')), MessagesPlaceholder(variable_name='history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='{question}'))]\n",
      "==history==\n",
      "[]\n",
      "content='Hi Sumin! How can I assist you today?'\n",
      "==history==\n",
      "[HumanMessage(content=\"Hi, I'm Sumin\"), AIMessage(content='Hi Sumin! How can I assist you today?')]\n",
      "content='That’s great! Jinju is known for its beautiful scenery and rich history, especially the Jinju Fortress and the Lantern Festival. Do you have any favorite places or activities in Jinju?'\n",
      "==history==\n",
      "[HumanMessage(content=\"Hi, I'm Sumin\"), AIMessage(content='Hi Sumin! How can I assist you today?'), HumanMessage(content='I live in Jinju, Korea'), AIMessage(content='That’s great! Jinju is known for its beautiful scenery and rich history, especially the Jinju Fortress and the Lantern Festival. Do you have any favorite places or activities in Jinju?')]\n",
      "content='Your name is Sumin! How can I help you today, Sumin?'\n",
      "{'history': [HumanMessage(content=\"Hi, I'm Sumin\"), AIMessage(content='Hi Sumin! How can I assist you today?'), HumanMessage(content='I live in Jinju, Korea'), AIMessage(content='That’s great! Jinju is known for its beautiful scenery and rich history, especially the Jinju Fortress and the Lantern Festival. Do you have any favorite places or activities in Jinju?'), HumanMessage(content='What is my name'), AIMessage(content='Your name is Sumin! How can I help you today, Sumin?')]}\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "  temperature=0.1,\n",
    "  model=\"gpt-4o-mini\",\n",
    "  # verbose=False #Chat 내역을 보여줌.\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "  llm=llm,\n",
    "  return_messages=True,\n",
    ")\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "  [\n",
    "    (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"), #history로 들어온 값을 모두 받기 위해\n",
    "    (\"human\", \"{question}\")\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(template)\n",
    "\n",
    "def load_memory(_):\n",
    "  print(\"==history==\")\n",
    "  print(memory.load_memory_variables({})[\"history\"])\n",
    "  return memory.load_memory_variables({})[\"history\"] #history로 저장된 값을 \n",
    "\n",
    "chain = RunnablePassthrough.assign(history=load_memory) | template | llm\n",
    "\n",
    "def invoke_chain(question):\n",
    "  result = chain.invoke({\"question\":question})\n",
    "  memory.save_context(\n",
    "    {\"input\":question},\n",
    "    {\"output\": result.content},\n",
    "  )\n",
    "  print(result)\n",
    "  \n",
    "invoke_chain(\"Hi, I'm Sumin\")\n",
    "invoke_chain(\"I live in Jinju, Korea\")\n",
    "invoke_chain(\"What is my name\")\n",
    "\n",
    "print(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 963, which is longer than the specified 600\n",
      "Created a chunk of size 774, which is longer than the specified 600\n",
      "Created a chunk of size 954, which is longer than the specified 600\n",
      "Created a chunk of size 922, which is longer than the specified 600\n",
      "Created a chunk of size 1168, which is longer than the specified 600\n",
      "Created a chunk of size 821, which is longer than the specified 600\n",
      "Created a chunk of size 700, which is longer than the specified 600\n",
      "Created a chunk of size 745, which is longer than the specified 600\n",
      "Created a chunk of size 735, which is longer than the specified 600\n",
      "Created a chunk of size 1110, which is longer than the specified 600\n",
      "Created a chunk of size 991, which is longer than the specified 600\n",
      "Created a chunk of size 990, which is longer than the specified 600\n",
      "Created a chunk of size 1741, which is longer than the specified 600\n",
      "Created a chunk of size 2001, which is longer than the specified 600\n",
      "Created a chunk of size 1900, which is longer than the specified 600\n",
      "Created a chunk of size 1130, which is longer than the specified 600\n",
      "Created a chunk of size 1799, which is longer than the specified 600\n",
      "Created a chunk of size 1690, which is longer than the specified 600\n",
      "Created a chunk of size 2364, which is longer than the specified 600\n",
      "Created a chunk of size 930, which is longer than the specified 600\n",
      "Created a chunk of size 1022, which is longer than the specified 600\n",
      "Created a chunk of size 1260, which is longer than the specified 600\n",
      "Created a chunk of size 795, which is longer than the specified 600\n",
      "Created a chunk of size 1293, which is longer than the specified 600\n",
      "Created a chunk of size 649, which is longer than the specified 600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6.1 Data loader and Splitter\n",
    "# loader(text, pdf, unstructured)\n",
    "# Splitter()\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader, PyPDFLoader, UnstructuredFileLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "\n",
    "# splitter #1\n",
    "# splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size=200, # 몇 글자를 기준으로 split 할 것인지\n",
    "#     chunk_overlap=50, # 앞의 글자를 얼마나 가져올 것인지\n",
    "# )\n",
    "\n",
    "# splitter #2\n",
    "# 위와의 차이는 자르는 character를 지정 가능.\n",
    "splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.docx\")\n",
    "\n",
    "# splitter 이용 1\n",
    "# docs = loader.load()\n",
    "# splitter.split_documents(docs)\n",
    "\n",
    "# splitter 이용 2\n",
    "loader.load_and_split(text_splitter=splitter)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
