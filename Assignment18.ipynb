{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Wikipedia에서 검색 or\n",
    "2. DuckDuckGo에서 검색\n",
    "2-1. 웹사이트의 텍스트를 스크랩하고 추출합니다.\n",
    "3. 리서치 결과를 .txt 파일에 저장하기\n",
    "4. 다음 쿼리로 에이전트를 실행합니다: \"Research about the XZ backdoor\" 라는 쿼리로 에이전트를 실행\n",
    "5. 에이전트는 Wikipedia 또는 DuckDuckGo에서 검색을 시도하고, DuckDuckGo에서 웹사이트를 찾으면 해당 웹사이트에 들어가서 콘텐츠를 추출한 다음 \n",
    "6. .txt 파일에 조사 내용을 저장하는 것으로 완료해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain.tools import BaseTool, DuckDuckGoSearchResults\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.retrievers import WikipediaRetriever\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from typing import Type\n",
    "import json\n",
    "import os\n",
    "\n",
    "def search_wiki(topic):\n",
    "    retriever = WikipediaRetriever(\n",
    "        top_k_result=4,\n",
    "    )\n",
    "    docs = retriever.invoke(topic)\n",
    "    return docs\n",
    "\n",
    "def search_duckduckgo(query):\n",
    "    search = DuckDuckGoSearchResults()\n",
    "    str = search.invoke(query)\n",
    "    find_s_string = \"link: https://\"\n",
    "    find_e_string = \"]\"\n",
    "    del_string = \"link: \"\n",
    "    s_index = str.find(find_s_string)\n",
    "    e_index = 0\n",
    "    urls = []\n",
    "    while s_index != -1:\n",
    "        s_index += e_index\n",
    "        e_index = str[s_index:].find(find_e_string) + s_index\n",
    "        urls.append(str[s_index + len(del_string):e_index])\n",
    "        s_index = str[e_index:].find(find_s_string)\n",
    "    return urls\n",
    "\n",
    "def save_content_to_txt(content, filename):\n",
    "    folder_dir = \"./.cache/agent\"\n",
    "    os.makedirs(folder_dir, exist_ok=True)\n",
    "    with open(f\"{folder_dir}/{filename}.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(content)\n",
    "\n",
    "def web_scrapping(urls):\n",
    "    print(\"urls: \", urls)\n",
    "    urls_list = urls.split(\"|\")\n",
    "    print(urls_list)\n",
    "    # loader = WebBaseLoader(urls_list)\n",
    "    # docs = loader.load()\n",
    "    # print(docs)\n",
    "    # return loader.load().page_content.replace(\"\\n\", \" \").repalce(\"  \", \" \")\n",
    "    return \"\"\n",
    "\n",
    "class WikipediaResearchSchema(BaseModel):\n",
    "    term: str = Field(\n",
    "        description=\"\"\"\n",
    "        The term you will search for on Wikipedia.\n",
    "        Enter only the word you're looking for.\n",
    "        Example term: the XZ backdoor\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "class WikipediaResearchTool(BaseTool):\n",
    "    name = \"WikipediaResearchTool\"\n",
    "    description = \"\"\"\n",
    "    Use this tool to search for something's information on Wikipedia.\n",
    "    It takes a term as an argument.\n",
    "    It returns the content searched on Wikipedia.\n",
    "    \"\"\"\n",
    "    args_schema: Type[WikipediaResearchSchema] = WikipediaResearchSchema\n",
    "\n",
    "    def _run(self, term):\n",
    "        return search_wiki(term)\n",
    "\n",
    "class DuckduckgoResearchSchema(BaseModel):\n",
    "    query: str = Field(\n",
    "        description=\"\"\"\n",
    "        The query you will search for.\n",
    "        Example query: Research about the XZ backdoor\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "class DuckduckgoResearchTool(BaseTool):\n",
    "    name = \"DuckduckgoResearchTool\"\n",
    "    description = \"\"\"\n",
    "    Use this tool to search for something's information.\n",
    "    It takes a query as an argument.\n",
    "    It receives a query, searches the website, and returns list of the website address(url).\n",
    "    \"\"\"\n",
    "    args_schema: Type[DuckduckgoResearchSchema] = DuckduckgoResearchSchema\n",
    "\n",
    "    def _run(self, query):\n",
    "        return search_duckduckgo(query)\n",
    "\n",
    "class WebScrapingSchema(BaseModel):\n",
    "    urls: str = Field(\n",
    "        description=\"\"\"\n",
    "        url list to extracts web information.\n",
    "        Example url list: 'https://en.wikipedia.org/wiki/URL|https://naver.com|https://daum.net'\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "class WebScrapingTool(BaseTool):\n",
    "    name = \"WebScrapingTool\"\n",
    "    description = \"\"\"\n",
    "    This tool accesses the specified URL and extracts information from them.\n",
    "    This tool receives URL list, scrapes the information from those URL, and returns web information.\n",
    "    \"\"\"\n",
    "    args_schema: Type[WebScrapingSchema] = WebScrapingSchema\n",
    "\n",
    "    def _run(self, urls):\n",
    "        return web_scrapping(urls)\n",
    "\n",
    "\n",
    "class SaveToTXTSchema(BaseModel):\n",
    "    content: str = Field(\n",
    "        description=\"Content to be saved as a text file.\"\n",
    "    )\n",
    "    filename: str = Field(\n",
    "        description=\"\"\"\"\n",
    "        Filename to save the content. A single word that contains the content.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "class SaveToTXTTool(BaseTool):\n",
    "    name = \"SaveToTXTTool\"\n",
    "    description = \"\"\"\n",
    "    This tool receives content and a filename and saves them as a text file.\n",
    "    \"\"\"\n",
    "    args_schema: Type[SaveToTXTSchema] = SaveToTXTSchema\n",
    "\n",
    "    def _run(self, content, filename):\n",
    "        save_content_to_txt(content, filename)\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "You are a research expert. \n",
    "When you get a research request, search DuckDuckGo.\n",
    "If the source lacks information, gather more from wikipedia.\n",
    "With DuckDuckGo, obtain the website's URL list.\n",
    "If you receive a list of URLs, you need to extract information from the websites through the URLs.\n",
    "Save all collected content as a text file. Just one file.\n",
    "Name the file with the key term.\n",
    "\"\"\"), \n",
    "(\"human\", \"research request: {request}\")\n",
    "])\n",
    "\n",
    "agent = initialize_agent(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    agent=AgentType.OPENAI_FUNCTIONS,\n",
    "    tools = [\n",
    "        WikipediaResearchTool(),\n",
    "        DuckduckgoResearchTool(),\n",
    "        WebScrapingTool(),\n",
    "        SaveToTXTTool()\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = template | agent\n",
    "chain.invoke({\"request\":\"Research about the XZ backdoor\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "# urls = ['https://www.wired.com/story/xz-backdoor-everything-you-need-to-know', 'https://www.wired.com/story/jia-tan-xz-backdoor/', 'https://www.akamai.com/blog/security-research/critical-linux-backdoor-xz-utils-discovered-what-to-know', 'https://securitylabs.datadoghq.com/articles/xz-backdoor-cve-2024-3094/']\n",
    "urls = ['https://www.akamai.com/blog/security-research/critical-linux-backdoor-xz-utils-discovered-what-to-know']\n",
    "\n",
    "print(\"urls: \", urls)\n",
    "loader = WebBaseLoader(urls, requests_per_second = 1)\n",
    "docs = loader.load()\n",
    "for doc in docs:\n",
    "    print(\"asldkfjasldkfjslk:\", doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request timed out!\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "# URL 리스트 정의\n",
    "# urls = ['https://www.akamai.com/blog/security-research/critical-linux-backdoor-xz-utils-discovered-what-to-know']\n",
    "urls = ['https://www.wired.com/story/xz-backdoor-everything-you-need-to-know', 'https://www.wired.com/story/jia-tan-xz-backdoor/', 'https://www.akamai.com/blog/security-research/critical-linux-backdoor-xz-utils-discovered-what-to-know', 'https://securitylabs.datadoghq.com/articles/xz-backdoor-cve-2024-3094/']\n",
    "\n",
    "\n",
    "# 문서 로딩을 위한 함수\n",
    "def load_documents():\n",
    "    loader = WebBaseLoader(urls)\n",
    "    return loader.load()\n",
    "\n",
    "# 타임아웃 처리를 위해 ThreadPoolExecutor 사용\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    future = executor.submit(load_documents)\n",
    "    docs = None\n",
    "    try:\n",
    "        # 10초 동안 결과를 기다림, 10초 넘으면 타임아웃 발생\n",
    "        docs = future.result(timeout=20)\n",
    "        print(\"Documents loaded successfully!\")\n",
    "        print(docs)\n",
    "    except concurrent.futures.TimeoutError:\n",
    "        print(docs)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
